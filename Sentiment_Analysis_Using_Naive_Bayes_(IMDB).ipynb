{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis Using Naive Bayes (IMDB).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhijunm/Text_Mining/blob/master/Sentiment_Analysis_Using_Naive_Bayes_(IMDB).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0kRAiXY2BtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#Tokenization of text\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "#remove stop-words\n",
        "from nltk.corpus import stopwords # library \n",
        "nltk.download('stopwords')\n",
        "all_stopwords = set(stopwords.words('english')) # set the language \n",
        "from typing import List"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnlAzuUs2JRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reading review data with panda frames \n",
        "reviews_data=pd.read_csv('IMDB Dataset.csv')\n",
        "reviews_data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE8na7zO2OuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentiment counts\n",
        "reviews_data['sentiment'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdFeDS-v2Sss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text: str) -> List[str]:\n",
        "    # Looking at the text we see that <br></br> which is HTML tag for line break can be a good splitter\n",
        "    # A sentence (atleast well structured) often has a full spot at the end. We use these two for word breaks\n",
        "    pattern1 = re.compile(\"<br /><br />|\\.\")\n",
        "    lines = re.split(pattern1, text)\n",
        "    # you can break a sentence into words using whitespace based split\n",
        "    tokens = []\n",
        "    for line in lines:\n",
        "        tokens += line.split(\" \")\n",
        "\n",
        "    # lowercase and remove any non-alphanumeric characters from tokens for normalize\n",
        "    normalized_tokens = [re.sub(r\"\\W+\", \"\", token.lower()) for token in tokens]\n",
        "    return  \" \".join([\n",
        "            token\n",
        "            for token in normalized_tokens\n",
        "            if token and token not in all_stopwords and len(tokens) > 1 \n",
        "        ])\n",
        "    \n",
        "\n",
        "  \n",
        "custom_review = \"I hated the film. It was a disaster. Poor direction, bad acting.\"\n",
        "custom_review_tokens = preprocess_text(custom_review)\n",
        "print(custom_review_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjwMh-P82WTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#apply preprocessing to review data\n",
        "reviews_data['review'] = reviews_data['review'].apply(preprocess_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAtHfosb2Zf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split the dataset  \n",
        "#train dataset\n",
        "train_reviews=reviews_data.review[:40000]\n",
        "train_sentiments=reviews_data.sentiment[:40000]\n",
        "#test dataset\n",
        "test_reviews=reviews_data.review[40000:45000]\n",
        "test_sentiments=reviews_data.sentiment[40000:45000]\n",
        "#validation (blind) dataset\n",
        "blind_reviews=reviews_data.review[45000:]\n",
        "blind_sentiments=reviews_data.sentiment[45000:]\n",
        "print(train_reviews.shape,train_sentiments.shape)\n",
        "print(test_reviews.shape,test_sentiments.shape)\n",
        "print(blind_reviews.shape,blind_sentiments.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2em5yDc2e6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CountVectorizer implements both tokenization and occurrence counting in a single class. Read more here https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "# You can also reuse the from scratch code we learnt in previous class\n",
        "# TfidfVectorizer Convert a collection of raw documents to a matrix of TF-IDF features. Equivalent to CountVectorizer followed by TfidfTransformer.\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#Count vectorizer with \n",
        "lower_count_thr = 100 # rare words/tokens\n",
        "upper_count_thr = 5000 # frequent/common tokens\n",
        "\n",
        "tv=TfidfVectorizer(min_df=lower_count_thr,max_df=upper_count_thr,binary=False,ngram_range=(1,1))\n",
        "#transformed train reviews\n",
        "tv_train_reviews=tv.fit_transform(train_reviews)\n",
        "#transformed test reviews\n",
        "tv_test_reviews=tv.transform(test_reviews)\n",
        "\n",
        "#transformed validation reviews\n",
        "tv_blind_reviews=tv.transform(blind_reviews)\n",
        "\n",
        "print('BOW_cv_train:',tv_train_reviews.shape)\n",
        "print('BOW_cv_test:',tv_test_reviews.shape)\n",
        "print('BOW_cv_blind:',tv_blind_reviews.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCMZpmwF2i9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now generate binary (true, false) labels from sentiment values. positive maps to 1, negative maps to 0\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb=LabelBinarizer()\n",
        "#transformed sentiment data\n",
        "sentiment_data=lb.fit_transform(reviews_data['sentiment'])\n",
        "print(sentiment_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcTVybe52llx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Spliting the sentiment data\n",
        "train_sentiments=sentiment_data[:40000]\n",
        "test_sentiments=sentiment_data[40000:45000]\n",
        "blind_sentiments=sentiment_data[45000:]\n",
        "print(train_sentiments.shape)\n",
        "print(test_sentiments.shape)\n",
        "print(blind_sentiments.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvJ6mHtV2103",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB()\n",
        "model.fit(tv_train_reviews,train_sentiments)\n",
        "labels = model.predict(tv_test_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcixgPKe29pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(test_sentiments,labels, normalize=True) * float(100)\n",
        "print('\\n****Test accuracy is',(acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxvRTrJW3A77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm_test = confusion_matrix(test_sentiments,labels)\n",
        "cm_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCGn_fei3D7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(cm_test,annot=True,fmt='d')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}